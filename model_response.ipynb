{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satyamgoyal/anaconda3/envs/llm_fairness/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../iol_2015/iol_2015.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# get prompt from txt file\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../iol_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/iol_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     43\u001b[0m     questions \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# write answers to respective text files\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_fairness/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../iol_2015/iol_2015.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"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\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCgSxYGVzpFYoWMiqQh31k888zI9uXxwMQ\"\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "\n",
    "model4 = ChatOpenAI(model=\"gpt-4\")\n",
    "model4o = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n",
    "model4om = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")\n",
    "model35s = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "model3o = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "model3h = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "model15p = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"{ling}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain4 = prompt | model4 | output_parser\n",
    "chain4o = prompt | model4o | output_parser\n",
    "chain4om = prompt | model4om | output_parser\n",
    "chain35s = prompt | model35s | output_parser\n",
    "chain3o = prompt | model3o | output_parser\n",
    "chain3h = prompt | model3h | output_parser\n",
    "\n",
    "year = 2005\n",
    "for i in range(2015, 2019):\n",
    "    year = str(i)\n",
    "    # get prompt from txt file\n",
    "\n",
    "    # write answers to respective text files\n",
    "    print(f\"Processing year {year}\")\n",
    "    # print(\"GPT-4\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_gpt4.txt', 'w') as f:\n",
    "    #     f.write(chain4.invoke({\"ling\": questions}))\n",
    "    # print(\"GPT-4o\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_gpt4o.txt', 'w') as f:\n",
    "    #     f.write(chain4o.invoke({\"ling\": questions}))\n",
    "    # print(\"GPT-4om\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_gpt4om.txt', 'w') as f:\n",
    "    #     f.write(chain4om.invoke({\"ling\": questions}))\n",
    "    # print(\"Claude 3.5s\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_claude3.5s.txt', 'w') as f:\n",
    "    #     f.write(chain35s.invoke({\"ling\": questions}))\n",
    "    # print(\"Claude 3o\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_claude3o.txt', 'w') as f:\n",
    "    #     f.write(chain3o.invoke({\"ling\": questions}))\n",
    "    # print(\"Claude 3h\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_claude3h.txt', 'w') as f:\n",
    "    #     f.write(chain3h.invoke({\"ling\": questions}))\n",
    "    print(\"Gemeni 1.5p\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_gemeni1.5p.txt', 'w') as f:\n",
    "    #     f.write(model15p.generate_content(questions).candidates[0].content.parts[0].text)\n",
    "    print(\"Done processing year {year}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
