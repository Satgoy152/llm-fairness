INFO:root:Generating outputs for 2 agents and 3 items with a uniform distribution using model gpt4om with temperature 0.7 and prompt type zero_shot...
INFO:root:Model first=ChatPromptTemplate(input_variables=['valuation'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['valuation'], input_types={}, partial_variables={}, template='{valuation}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1530b45b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1530be760>, root_client=<openai.OpenAI object at 0x1530aa3d0>, root_async_client=<openai.AsyncOpenAI object at 0x1530b4610>, model_name='gpt-4o-mini-2024-07-18', model_kwargs={}, openai_api_key=SecretStr('**********'))] last=StrOutputParser() initialized
INFO:root:Starting output 1...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 1 generated
INFO:root:Starting output 2...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 2 generated
INFO:root:Starting output 3...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 3 generated
INFO:root:Starting output 4...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 4 generated
INFO:root:Starting output 5...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 5 generated
INFO:root:Starting output 6...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 6 generated
INFO:root:Starting output 7...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 7 generated
INFO:root:Starting output 8...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 8 generated
INFO:root:Starting output 9...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 9 generated
INFO:root:Starting output 10...
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:root:Output 10 generated
INFO:root:Outputs generated for 2 agents and 3 items with a uniform distribution using model first=ChatPromptTemplate(input_variables=['valuation'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['valuation'], input_types={}, partial_variables={}, template='{valuation}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1530b45b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1530be760>, root_client=<openai.OpenAI object at 0x1530aa3d0>, root_async_client=<openai.AsyncOpenAI object at 0x1530b4610>, model_name='gpt-4o-mini-2024-07-18', model_kwargs={}, openai_api_key=SecretStr('**********'))] last=StrOutputParser() with temperature 0.7 and prompt type zero_shot...
INFO:root:Extracting and evaluating outputs...
INFO:root:Envy calculated
INFO:root:Envy evaluated and saved
